{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from pyspark import SQLContext, SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from distkeras.trainers import *\n",
    "from distkeras.predictors import *\n",
    "from distkeras.transformers import *\n",
    "from distkeras.evaluators import *\n",
    "from distkeras.utils import *\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and labels\n",
    "x=np.load('MCx.npy')\n",
    "y=np.load('MCy.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rank of each feature\n",
    "R=[]\n",
    "for h in range(x.shape[1]):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=2, n_init=10)\n",
    "    ff=kmeans.fit_predict(x[:,h].reshape(-1,1))\n",
    "    r=metrics.homogeneity_score(y,ff) #Use the homogeneity score as a rank of the feature\n",
    "    R.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack((np.arange(43).reshape(-1,1),((np.array(R)-0.5)*2).reshape(-1,1)))[argsort(np.array(R)),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arrange feature accroding to thier ranks\n",
    "Rnk=np.argsort(np.array(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the cross-validation splitter\n",
    "kfolds=StratifiedKFold(n_splits=5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Per each set of ranks, use cross-validation to calculate accuracy.\n",
    "smr=[]\n",
    "for j in range(Rnk.shape[0]):\n",
    "    fd=x[:,Rnk[j:]]\n",
    "    pp=0\n",
    "    for train,test in kfolds.split(fd,y):\n",
    "        dff = map(lambda x: (int(float(x[-1])), Vectors.dense(x[:-1])),np.hstack((fd[train],y[train].reshape(-1,1))))\n",
    "        TrD = spark.createDataFrame(dff,schema=[\"label\", \"features\"])\n",
    "        dff = map(lambda x: (int(float(x[-1])), Vectors.dense(x[:-1])),np.hstack((fd[test],y[test].reshape(-1,1))))\n",
    "        TsD = spark.createDataFrame(dff,schema=[\"label\", \"features\"])\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128,input_dim=fd.shape[1],activation='relu',use_bias=True))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(64,activation='relu',use_bias=True))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(32,activation='relu',use_bias=True))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(9,activation='softmax',use_bias=True)) #The number of neurons is equal to the number of classes\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "        trainer = SingleTrainer(keras_model=model, worker_optimizer='adam', loss='categorical_crossentropy', num_epoch=0)\n",
    "        trained_model = trainer.train(TrD)\n",
    "        st = time.time()\n",
    "        predictor = ModelPredictor(keras_model=trained_model)\n",
    "        et+=time.time()-st\n",
    "        ts=np.array(predictor.predict(TsD).collect())\n",
    "        pp=pp+metrics.accuracy_score(y[test],ts)\n",
    "    pp=pp/kfolds.n_splits\n",
    "    gg=predictor.predict(TsD).collect()\n",
    "    smr.append([j, pp, et/2.540047]) #Calculate the time requires to predict a label per each object in uS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
